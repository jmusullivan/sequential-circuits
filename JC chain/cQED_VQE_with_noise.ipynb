{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3895e294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from scipy.stats import unitary_group\n",
    "from numpy import array\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff919e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:-0.03201468720387668, Time:0.10565400123596191\n",
      "Epoch: 100, Loss:-3.6785096406708204, Time:9.643810749053955\n",
      "Epoch: 200, Loss:-3.685018382711881, Time:19.397751092910767\n",
      "Epoch: 300, Loss:-3.6876212959540613, Time:29.797627925872803\n",
      "Epoch: 400, Loss:-3.6945061055881068, Time:39.58246302604675\n",
      "Epoch: 500, Loss:-3.6959788334162886, Time:49.80606269836426\n",
      "Epoch: 600, Loss:-3.7094379232165844, Time:59.877628803253174\n",
      "Epoch: 700, Loss:-3.7101195159063725, Time:69.29267191886902\n",
      "Epoch: 800, Loss:-3.7048019622380104, Time:78.92608094215393\n",
      "Epoch: 900, Loss:-3.7759172602875513, Time:88.75159192085266\n",
      "Epoch: 1000, Loss:-3.802116858807922, Time:98.60882782936096\n",
      "Epoch: 1100, Loss:-3.9101177780933494, Time:109.04788303375244\n",
      "Epoch: 1200, Loss:-3.9528694364661443, Time:119.50072383880615\n",
      "Epoch: 1300, Loss:-3.978961392546126, Time:129.33133792877197\n",
      "Epoch: 1400, Loss:-3.9992598093006135, Time:139.09415888786316\n",
      "Epoch: 1500, Loss:-4.013397825083274, Time:148.8583607673645\n",
      "Epoch: 1600, Loss:-4.023449621914632, Time:158.19258975982666\n",
      "Epoch: 1700, Loss:-4.029266897481838, Time:168.11578965187073\n",
      "Epoch: 1800, Loss:-4.033728127280629, Time:177.3364758491516\n",
      "Epoch: 1900, Loss:-4.037377185368672, Time:186.33253407478333\n"
     ]
    }
   ],
   "source": [
    "def raising_operator(dim):\n",
    "    matrix = np.zeros([dim,dim]) *1j\n",
    "    for i in range(dim-1):\n",
    "        matrix[i+1][i] = np.sqrt(i+1)\n",
    "    return matrix\n",
    "\n",
    "def lowering_operator(dim):\n",
    "    matrix = np.zeros([dim,dim])* 1j\n",
    "    for i in range(dim-1):\n",
    "        matrix[i][i+1] = np.sqrt(i+1)\n",
    "    return matrix\n",
    "\n",
    "def Hstatic(dim_c, dim_q, w_t, w_c, K, A, x, X):\n",
    "    ar = raising_operator(dim_c)\n",
    "    al = lowering_operator(dim_c)\n",
    "    br = raising_operator(dim_q)\n",
    "    bl = lowering_operator(dim_q)\n",
    "    \n",
    "    H_o = w_c*np.kron(ar@al,np.identity(dim_q)) + K/2*np.kron(ar@ar@al@al,np.identity(dim_q))\n",
    "    H_t = w_t*np.kron(np.identity(dim_c),br@bl) + A/2*np.kron(np.identity(dim_c),br@br@bl@bl)\n",
    "    H_i = x*np.kron(ar@al,br@bl)+X/2*np.kron(ar@ar@al@al,br@bl,)\n",
    "    Hstatic = H_o + H_t + H_i\n",
    "    return Hstatic\n",
    "\n",
    "def cQED_hamiltonian_terms(dim_c, dim_q, w_t, w_c, K, A, x, X):\n",
    "    ar = raising_operator(dim_c)\n",
    "    al = lowering_operator(dim_c)\n",
    "    br = raising_operator(dim_q)\n",
    "    bl = lowering_operator(dim_q)\n",
    "    \n",
    "    h_1  = np.kron(al, np.identity(dim_q))+np.kron(ar, np.identity(dim_q))\n",
    "    h_2 = np.kron(np.identity(dim_c),bl)+np.kron(np.identity(dim_c),br)\n",
    "    h_1c  = ((np.kron(al, np.identity(dim_q)))-np.kron(ar, np.identity(dim_q)))*1j\n",
    "    h_2c = ((np.kron(np.identity(dim_c),bl))-np.kron(np.identity(dim_c),br))*1j \n",
    "    h_d = Hstatic(dim_c, dim_q, w_t, w_c, K, A, x, X)\n",
    "    return h_1, h_2, h_1c, h_2c, h_d\n",
    "\n",
    "\n",
    "def hamiltonian(w1, w1_c, w2, w2_c):\n",
    "    # Omega_c: cavity frequency\n",
    "    # Omega_a: atom frequency\n",
    "    # g: coupling strength\n",
    "    H_1 = w1 * torch.tensor(h_1*10**6, dtype=torch.complex128,)\n",
    "    H_1c = w1_c * torch.tensor(h_1c*10**6 , dtype=torch.complex128)\n",
    "    H_2 = w2 * torch.tensor(h_2*10**6, dtype=torch.complex128)\n",
    "    H_2c = w2_c * torch.tensor(h_2c*10**6, dtype=torch.complex128)\n",
    "\n",
    "    H = H_1 + H_1c + H_2 + H_2c + torch.tensor(h_d)\n",
    "    return H\n",
    "\n",
    "def projector(dim_c, dim_b):\n",
    "    p = torch.zeros([dim_c*2, dim_c*2],dtype=torch.complex128,)\n",
    "    for i in range (2*dim_b, 2*dim_c):\n",
    "        p[i][i] = dim_c/(dim_c-dim_b) # normalize to match the fid computer \n",
    "    return p\n",
    "import quimb as qu\n",
    "import quimb.tensor as qtn\n",
    "def noisy_tensors(n_bond_lvl,L=1,error_rate =[0,0,0]):\n",
    "    \n",
    "    # we approximate Linbladian evolution as a unitary evolution followed by the collapse operators\n",
    "    # let's define the collapse operators as a MPO\n",
    "    \n",
    "    tlist = []\n",
    "    p_rel =  lowering_operator(2)\n",
    "    p_dep =  raising_operator(2) @ lowering_operator(2)\n",
    "    b_rel = lowering_operator(n_bond_lvl)\n",
    "    \n",
    "    p_rel_tensor = np.zeros([4,2,2])+0j\n",
    "    p_rel_tensor[0] = np.eye(2)\n",
    "    p_rel_tensor[1] = error_rate[0] *p_rel\n",
    "    p_rel_tensor[2] = error_rate[0] * p_rel@np.conj(p_rel.T) \n",
    "    p_rel_tensor[3] = error_rate[0] *np.eye(2)\n",
    "    \n",
    "    pc_rel_tensor = np.zeros([4,2,2])+0j\n",
    "    pc_rel_tensor[0] = np.eye(2)\n",
    "    pc_rel_tensor[1] = np.conj(p_rel)\n",
    "    pc_rel_tensor[2] = -1/2*np.eye(2)\n",
    "    pc_rel_tensor[3] = -1/2*(p_rel@ np.conj(p_rel.T) )\n",
    "    \n",
    "    p_dep_tensor = np.zeros([4,2,2])+0j\n",
    "    p_dep_tensor[0] = np.eye(2)\n",
    "    p_dep_tensor[1] = error_rate[1] *p_dep\n",
    "    p_dep_tensor[2] = error_rate[1] * p_dep@np.conj(p_dep.T) \n",
    "    p_dep_tensor[3] = error_rate[1] *np.eye(2)\n",
    "    \n",
    "    pc_dep_tensor = np.zeros([4,2,2])+0j\n",
    "    pc_dep_tensor[0] = np.eye(2)\n",
    "    pc_dep_tensor[1] = np.conj(p_dep)\n",
    "    pc_dep_tensor[2] = -1/2*np.eye(2)\n",
    "    pc_dep_tensor[3] = -1/2*(p_dep@ np.conj(p_dep.T) )\n",
    "    \n",
    "    b_rel_tensor = np.zeros([4,n_bond_lvl,n_bond_lvl])+0j\n",
    "    b_rel_tensor[0] = np.eye(n_bond_lvl)\n",
    "    b_rel_tensor[1] = error_rate[2] *b_rel\n",
    "    b_rel_tensor[2] = error_rate[2] * b_rel@np.conj(b_rel.T) \n",
    "    b_rel_tensor[3] = error_rate[2] *np.eye(n_bond_lvl)\n",
    "    \n",
    "    bc_rel_tensor = np.zeros([4,n_bond_lvl,n_bond_lvl])+0j\n",
    "    bc_rel_tensor[0] = np.eye(n_bond_lvl)\n",
    "    bc_rel_tensor[1] = np.conj(b_rel)\n",
    "    bc_rel_tensor[2] = -1/2*np.eye(n_bond_lvl)\n",
    "    bc_rel_tensor[3] = -1/2*(b_rel@ np.conj(b_rel.T) )\n",
    "    \n",
    "    for i in range (L):\n",
    "        inds_d = f'a_rel{i}',f'p_rel{i}',f'p_dep{i}'\n",
    "        t = qtn.Tensor(p_rel_tensor,inds_d,tags=f'site{i}')\n",
    "        inds_d = f'a_rel{i}',f'pc_rel{i}',f'pc_dep{i}'\n",
    "        tc = qtn.Tensor(pc_rel_tensor,inds_d,tags=f'site{i}')\n",
    "        tlist.append(t&tc)\n",
    "        \n",
    "        inds_d =f'a_dep{i}',f'p_dep{i}',f'p_out{i}'\n",
    "        t = qtn.Tensor(p_dep_tensor,inds_d,tags=f'site{i}')\n",
    "        inds_d =f'a_dep{i}',f'pc_dep{i}',f'pc_out{i}'\n",
    "        tc = qtn.Tensor(pc_dep_tensor,inds_d,tags=f'site{i}')\n",
    "        tlist.append(t&tc)\n",
    "        # add in error models for cavity\n",
    "        inds_d =f'b{i}',f'b_rel{i}',f'b_out{i}'\n",
    "        t = qtn.Tensor(b_rel_tensor,inds_d,tags=f'site{i}')\n",
    "        inds_d =f'b{i}',f'bc_rel{i}',f'bc_out{i}'\n",
    "        tc = qtn.Tensor(bc_rel_tensor,inds_d,tags=f'site{i}')\n",
    "        tlist.append(t&tc)\n",
    "\n",
    "    #tn = tlist[0]\n",
    "    #for i in range (1,len(tlist)):\n",
    "    #    tn = tn&tlist[i]\n",
    "    return ((tlist[0]&tlist[1])^all).data, (tlist[2]^all).data\n",
    "\n",
    "def objective_function_unitary_sythesis(parameters, dt, n_steps, U_target):\n",
    "    w1, w1_c, w2, w2_c = parameters\n",
    "    \n",
    "    U = torch.eye(dim_c*dim_q, dtype=torch.complex128, requires_grad=True)  # Initialize the unitary\n",
    "    for i in range(n_steps):\n",
    "        H = hamiltonian(w1[i], w1_c[i], w2[i], w2_c[i])\n",
    "        U = U@torch.matrix_exp(-1j * H * dt)\n",
    "    #print(U)\n",
    "    \n",
    "    fidelity = torch.abs(torch.trace(torch.matmul(U_target, U))) / (dim_c*dim_q)\n",
    "    loss = 1 - fidelity\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def objective_function(parameters, dt, n_steps,noisy):\n",
    "    w1, w1_c, w2, w2_c = parameters\n",
    "    \n",
    "    U = torch.eye(dim_c*dim_q, dtype=torch.complex128, requires_grad=True)  # Initialize the unitary\n",
    "    for i in range(n_steps):\n",
    "        H = hamiltonian(w1[i], w1_c[i], w2[i], w2_c[i])\n",
    "        U = U@torch.matrix_exp(-1j * H * dt)\n",
    "    #print(U)\n",
    "    ##loss = 1 - fidelity\n",
    "    \n",
    "    penalty = 1 - torch.abs(torch.trace(torch.matmul(projector(dim_c, dim_b), U))) / (dim_c*dim_q)\n",
    "    mpo = create_mpo(L, ham, burn_in)\n",
    "    \n",
    "    \n",
    "    if dim_c != dim_b:\n",
    "        #loss = evaluate_mps_mpo_noisy(U, mpo , L, burn_in, noisy) + .2*penalty\n",
    "        loss = evaluate_mps_mpo_noisy(U, mpo , L, burn_in,noisy) + .2 * penalty\n",
    "    else:\n",
    "        loss = evaluate_mps_mpo_noisy(U, mpo , L, burn_in,noisy)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Function to contract MPS and MPO tensors and compute the expectation value\n",
    "from hamiltonian import model_mpo\n",
    "\n",
    "L, burn_in = 10,5\n",
    "J, V, h = 1, .5, -1\n",
    "ham = torch.tensor(model_mpo.sd_ising(J, V, h, L - burn_in),dtype=torch.complex128)\n",
    "\n",
    "def create_mpo(L, ham, burn_in = 0):\n",
    "    mpo = []\n",
    "    for _ in range (0, burn_in):\n",
    "        mpo.append(torch.eye(10,dtype=torch.complex128).reshape(2,5,2,5))\n",
    "    for _ in range (burn_in, L):\n",
    "        mpo.append(ham)\n",
    "    return mpo\n",
    "\n",
    "mpo = create_mpo(L, ham, burn_in)\n",
    "\n",
    "def evaluate_mps_mpo_noisy(unitary, mpo , L, burn_in = 0,noise_tensors = None):\n",
    "    \n",
    "    # assume this is a translational invariant MPS\n",
    "    isometry = unitary.reshape(dim_c,2,dim_c,2) # dim_c,dim_q\n",
    "    \n",
    "    # define boundary condition \n",
    "    bdry_mps = torch.eye(dim_c,dtype=torch.complex128)[0] \n",
    "    bdry_mpo_l = torch.eye(5,dtype = torch.complex128)[-1]#shape (2,5,2,5)\n",
    "    bdry_mpo_r = torch.eye(5,dtype = torch.complex128)[0]\n",
    "    \n",
    "    # Contract input state with the first isometry; ckr: p_out, pc_out,\n",
    "    if noise_tensors == None or burn_in !=0 :\n",
    "        mps_state = torch.einsum(\"a, abcd, i, ilkj, o, jrdo, bl-> ckr\", bdry_mps, isometry, bdry_mps, torch.conj(isometry), bdry_mpo_l, mpo[0],torch.tensor([[1,0],[0,0]],dtype = torch.complex128))\n",
    "\n",
    "    else:\n",
    "        #p_rel0', 'pc_rel0', 'p_out0', 'pc_out0'; 'b_rel0', 'b_out0', 'bc_rel0', 'bc_out0'\n",
    "        p_noise, b_noise = noise_tensors\n",
    "        mps_state = torch.einsum(\"a, abcd, i, ilkj, o, jrdo, vw, blvw -> ckr\", bdry_mps, isometry, bdry_mps, torch.conj(isometry), bdry_mpo_l, mpo[0],torch.tensor([[1,0],[0,0]],dtype = torch.complex128),torch.tensor(p_noise,dtype = torch.complex128))\n",
    "        mps_state = torch.einsum(\"ckr,cvkw -> vwr\", mps_state, torch.tensor(b_noise,dtype = torch.complex128))\n",
    "\n",
    "    # Contract the MPS state with the MPO and the remaining isometries\n",
    "    for i in range(1, L):\n",
    "        if noise_tensors == None or i < burn_in:\n",
    "            mps_state = torch.einsum(\"aip, abcd, ilkj, jrdp, bl-> ckr\", mps_state,  isometry, torch.conj(isometry), mpo[i],torch.tensor([[1,0],[0,0]],dtype = torch.complex128))\n",
    "        else:\n",
    "            #p_rel0', 'pc_rel0', 'p_out0', 'pc_out0'; 'b_rel0', 'b_out0', 'bc_rel0', 'bc_out0'\n",
    "            p_noise, b_noise = noise_tensors\n",
    "            mps_state = torch.einsum(\"aip, abcd, ilkj, jrdp, vw, blvw-> ckr\", mps_state,  isometry, torch.conj(isometry), mpo[i],torch.tensor([[1,0],[0,0]],dtype = torch.complex128),torch.tensor(p_noise,dtype = torch.complex128))\n",
    "            mps_state = torch.einsum(\"ckr,cvkw -> vwr\", mps_state, torch.tensor(b_noise,dtype = torch.complex128))\n",
    "\n",
    "    # Contract the final MPS tensor with its conjugate\n",
    "    result = torch.real(torch.einsum(\"aap, p-> \", mps_state,bdry_mpo_r))\n",
    "    return result\n",
    "\n",
    "# Create the MPO observable (identity)\n",
    "mpo = create_mpo(6, ham, 0)\n",
    "\n",
    "# Evaluate the MPS with the MPO observable\n",
    "\n",
    "dim_b = 4 # bond dimension = useful cavity levels\n",
    "dim_c = dim_b+6 # of cavity/oscillator levels\n",
    "dim_q = 2 # of qubit levels; which is usually set to 2\n",
    "\n",
    "#System Parameter; taken from Yale's\n",
    "w_c = 0#2*np.pi*4452*10**6\n",
    "w_t = 0#2*np.pi*5664*10**6\n",
    "K = -2*np.pi*3.7*1000\n",
    "A = -2*np.pi*236*10**6\n",
    "x = -2*np.pi*2.139 *10**6#chi\n",
    "X = -2*np.pi*19*1000\n",
    "\n",
    "h_1, h_2, h_1c, h_2c, h_d = cQED_hamiltonian_terms(dim_c, dim_q, w_t, w_c, K, A, x, X)\n",
    "\n",
    "# Time step and number of steps\n",
    "dt = 10 * 10**(-9)\n",
    "n_steps = 100\n",
    "\n",
    "# Initial parameters\n",
    "w1 = Variable(torch.tensor(np.random.random(n_steps), dtype=torch.float64), requires_grad=True)\n",
    "w1_c = Variable(torch.tensor(np.random.random(n_steps) , dtype=torch.float64), requires_grad=True)\n",
    "w2 = Variable(torch.tensor(np.random.random(n_steps), dtype=torch.float64), requires_grad=True)\n",
    "w2_c = Variable(torch.tensor(np.random.random(n_steps) , dtype=torch.float64), requires_grad=True)\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = optim.Adam([w1, w1_c, w2, w2_c], lr=0.2)\n",
    "\n",
    "noisy = noisy_tensors(dim_c,1,[1/170,1/43,1/2700]) #set up the noise strength\n",
    "\n",
    "n_epochs = 2000\n",
    "import time\n",
    "t0 = time.time()\n",
    "for epoch in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    loss = objective_function([w1, w1_c, w2, w2_c], dt, n_steps,noisy)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    with torch.no_grad():\n",
    "        for param in [w1, w1_c, w2, w2_c]:\n",
    "            param.clamp_(-10, 10)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch: {epoch}, Loss:{loss}, Time:{time.time()-t0}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffadb59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
